# 操作系统大作业报告——I/O调度

组员（按姓氏排序）：洪宇睿，卢敏思，覃思中

组长：朱景熙

## 一．选题概述

### 基本环境

ubuntu20.04，qemu（注意，非16和18版本的ubuntu可能会导致无法引入交叉编译的编译器导致测试失败）

 ### 操作系统

xv6-2020版：2020版的xv6在文件系统的架构上做了较大的改变，并且在一些系统调用的具体实现上也与之前版本有所差异，给I/O调度的实现造成了较大的困难，其主要改动可以分为以下几点：

* 虚拟磁盘上由原来的ide磁盘变为新版的virtio磁盘，在IO的具体实现上去掉了直观的请求队列，而是采用更为复杂的数据结构存储请求，给阅读带来了较大的困难。但从理论上而言，virtio磁盘在性能上会明显优于ide磁盘，这是因为ide是全软件模拟，virtio是半虚拟化驱动。virtio改造了虚拟机，告诉虚拟机自己就是一台虚拟机，可以直接和虚拟化层通讯，以此来提高通讯效率。
* 在日志层中，新版xv6实现了对于多个fs系统的并发调用，而在旧版xv6中则不允许并发会话，日志块被写操作进程独占，其他写操作的进程必须等待当前会话结束，导致磁盘请求队列中的写操作都来自同个进程。
* 此外，新版xv6在一些系统调用函数上进行了改动，包括printf()，wait()，exit()等，在具体的实现上与旧版略有差异，不过影响不大。

### IO调度和磁盘调度

IO调度和磁盘调度感观上来看较为相似，但对于较为复杂的操作系统来说，这是两个不同的概念。

IO调度的应用场景是向块设备写入数据块或是从块设备读出数据块时，IO请求要先进入IO队列，等待调度。这个IO队列和调度的目标是针对某个块设备而言的，也就是每个块设备都有一个独立的IO队列。但块设备并不是指物理磁盘。假如一个磁盘被分为多个分区，每个分区作为一个块设备，则每个块设备都可以有自己独立的队列和调度方法，IO调度的基本算法包括下面四种：

* **NOOP**
  最朴素的算法思路，即先进先出(FIFO)，不改变请求的顺序。NOOP维护了一个FIFO链表的链表头，当检测到IO请求时，就会将请求加入到链表的后面。而FIFO链表前面的请求则会被移到系统的请求队列(request_queue)中

* **Deadline**
  Deadline使用两种不同的数据结构管理请求，其中FIFO队列使用双向链表，按照先后顺序排列，而排序队列则是基于红黑树的数据结构，根据块号来进行排序。它把请求按照读写方向分为读和写两类，因此同时维护了四个队列。
  在选择排序队列时，方向默认为读，只有当写请求队列被忽略两次时，才会选择写请求队列，这是因为读请求一般都是同步的，会阻塞进程运行，而写请求可以适当延迟。
  派发请求时，按扇区号从小到大发派请求，当当前排序队列中已经没有请求时，再选择另一个方向排序队列派发。
  每当选择好队列后，算法会检查FIFO队列中是否有请求超时，若超时，则把磁头移动到请求超时的位置，直接派发请求，并且从该位置开始继续按照块号派发，以有效防止一些I/O进程始终得不到回应。
  这种算法对小数据量随机读写性能较好，而对大数据量顺序读写效果不佳。

* **Anticipatory**
  Anticipatory算法是Deadline算法的进一步优化，在Deadline的基础上增加了预测的功能，当同步请求处理完毕后，若调度算法判定该进程会发出更多位置相近的请求，则等待一段时间再进行处理。
  其主要难点在于如何根据进程的历史行为判断是否需要等待，linux中采用历史统计的想法，当进程思考时间小于等待时间且寻址距离小于一定值时才会等待。
  整体而言相较于Deadline对大文件连续读写有着更好的性能。

* **CFQ**
  CFQ引入I/O优先级的概念，根据进程的I/O优先级进行调度，实现进程间的I/O公平性，并同样引入Anticipatory的优点，对于一些同步I/O操作也进行等待，以获得更好的性能，是现在linux的默认调度算法。此外，也有一种说法认为CFQ算法仅为按照IO请求的地址进行排序，而不是按照先来后到的顺序来进行响应，这里由于各进程并没有设计优先级，因此我们采用简化的CFQ算法。

而磁盘调度算法，则是针对磁盘进行的调度，为每个磁盘设立一个等待队列，根据磁盘进行调度，主要考虑减少其柱面定位时间，其基本方法包括下面四种：

* **先来先服务算法（FCFS）First Come First Service**
  与FIFO类似，采用简单的等待队列，但没有对柱面定位进行优化，在磁盘的访问请求比较多的情况下，会导致平均寻道时间较长，但各进程得到服务的响应时间变化幅度较小。该算法看起来很公平，没有饥饿（所有请求都按顺序服务）。

* **最短寻道时间优先算法（SSTF） Shortest Seek Time First**
  SSTF算法基本思想是最接近当前磁盘磁头位置的磁道应首先服务，以使每次的寻道时间最短，但这样的方法不能保证平均寻道时间最短，且响应时间的变化幅度很大。在服务请求很多的情况下，可能会造成较为严重的饥饿现象。由于响应时间的高度差异，它们缺乏可预测性，因为它有利于容易到达的请求并忽略远处的进程。

* **扫描算法（SCAN）电梯调度**
  扫描算法不仅考虑到欲访问的磁道与当前磁道的距离，更优先考虑的是磁头的当前移动方向，先按一个方向遍历，再反复折回，与电梯的运行较为类似，因此也称为电梯调度算法。此算法解决了饥饿的问题，但对于两侧访问频率仍然更高。结果导致中端的请求得到了更多的服务。SCAN 算法没有饥饿，但这种算法是不公平的，因为它会导致磁头刚访问的柱面等待时间过长。

* **循环扫描算法（CSCAN）**
  循环扫描算法是对扫描算法的改进，规定磁头单向移动。当磁头移到最外侧时，立即返回最里侧，将最大磁道号紧接着最小磁道号构成循环，进行扫描，在解决饥饿问题的同时还可以避免两侧访问频率过高。适用于中等至重载。它提供了更好的响应时间和统一的等待时间。
  
以上两类调度算法在如Linux等操作系统中功能不同，因此需要分开实现，但在xv6操作系统中，由于主要设备仅为一个半虚拟磁盘virtio disk，因此两者功能较为重合，其中NOOP与FCFS基本一致，CFQ与CSCAN的实现也基本一致，因此我们接下来主要完成了NOOP，CFQ，SSTF的调度算法，并完成了对deadline算法的尝试。

## 二．环境配置

### 1.macOS:
a) 首先下载visual box
https://www.aliyue.net/9072.html
注意如果在双击pkg文件后安装步骤中报错，请检查“系统偏好”——>“安全性与隐私”并允许pkg安装所用的测试程序在系统中运行。

b) 下载ubuntu20.4 系统iso文件，配置ubuntu系统
虚拟机配置教程：https://jingyan.baidu.com/article/295430f1c47e9e0c7e00502f.html
所需的系统iso文件链接：https://mirrors.tuna.tsinghua.edu.cn/ubuntu-releases/
选择20.04/——> ![img](file:///C:/Users/qinsz/AppData/Local/Temp/msohtmlclip1/01/clip_image002.png)文件安装
进一步功能参考下面的备注部分

c）之后的部分按照ubuntu系统的内容进行配置。
备注：设置虚拟机和macOS共享文件夹：
https://blog.csdn.net/qq_20044689/article/details/52434028
注意a.原链接中文件的权限在“显示简介”中，要对“共享的文件夹”打钩，并且在共享与权限中添加”everyone”“读与写”
![img](file:///C:/Users/qinsz/AppData/Local/Temp/msohtmlclip1/01/clip_image004.png)
图1 共享文件夹权限设置

b.下图sudo mount指令如果提示no such device，重启虚拟机即可
最终效果：
![img](file:///C:/Users/qinsz/AppData/Local/Temp/msohtmlclip1/01/clip_image006.jpg)
图2共享文件夹设置成功的效果

### 2.ubuntu: 
相当于在ubuntu系统中开了一个qemu小模拟器，并把xv6作为该模拟机的操作系统

a)利用刚才创建的共享文件夹，将xv6传入虚拟机中（非虚拟机可跳过）

b)在系统命令行运行以下两行命令，更新软件包列表
![img](file:///C:/Users/qinsz/AppData/Local/Temp/msohtmlclip1/01/clip_image008.png)
来源：https://pdos.csail.mit.edu/6.828/2021/tools.html

(c) 在命令行中进入xv6所在的文件夹目录，运行make命令进行编译，如果未报错，则在命令行执行make qemu命令。如果出现问题，可以参考http://blog.csdn.net/qq_38163755/article/details/78136947
输出ls，显示当前根目录下的所有文件，即说明环境配置成功。
![img](file:///C:/Users/qinsz/AppData/Local/Temp/msohtmlclip1/01/clip_image010.png)
图3配置xv6成功的效果

(d) 添加代码与demo：
将写好的xxx.c文件放入user文件夹中，并在makefile文件的UPROGS变量中添加程序名，再次make qemu后输入命令即可调用。（使用ls可以看到刚才加入的xxx.c已经成功成为了操作系统的命令之一）
![img](file:///C:/Users/qinsz/AppData/Local/Temp/msohtmlclip1/01/clip_image012.png)图4添加demo程序

### 3.Windows
在Windows中通过Windows Subsystem for Linux（简称WSL）兼容层运行原生Linux二进制可执行文件（ELF格式），其余操作与Ubuntu几乎一致。

## 三．实现思路

### 1.NOOP：

想了解在磁盘上NOOP的具体实现，首先需要了解virtio磁盘的实现机制，为解决全虚拟化在性能上的问题，半模拟技术应运而生。它构造了一种虚拟化环境所独有的存储设备，因此半虚拟化需要在虚拟机内部安装特定的驱动程序才能正常驱使该设备进行工作。通常我们称虚拟机内部的驱动为前端驱动，称负责实现其功能模拟的程序(KVM平台下即为qemu程序)为后端程序。而在xv6系统中我们无法对后端程序进行修改，因此只能在前端进行一些操作来实现。

virtio 层属于控制层，负责设备跟OS之间的通知和控制，具体来说，在xv6的virtio磁盘中，采用中断方式实现后端到前端的通知，并通过IO环(ring)数据结构进行数据的共享。

磁盘disk的具体数据结构如下所示：

```c
static struct disk {
  //virtio驱动程序和设备主要通过一组
  //RAM中的结构进行通信。pages[]分配该内存。pages[]是
  //全局的（而不是调用kalloc（）），因为它必须由
  //物理内存的两个连续页组成
  char pages[2*PGSIZE];

  //pages[]被划分为三个区域（DMA描述符，希望处理的描述符，和已经使用的描述符）

  //pages[]的第一个区域是DMA的集合（不是ring）
  //描述符，驱动程序用它告诉设备在何处读取
  //和写入单个磁盘操作。一共有NUM个描述符。
  //大多数命令都由一个“链”（链表）和几个
  //描述符组成。
  struct virtq_desc *desc;

  //接下来是一个ring结构，驱动程序在其中写入驱动程序希望设备处理
  //的描述符编号。它只
  //包括每个链的头描述符。这个ring里有
  //NUM个元素。
  struct virtq_avail *avail;

  //最后也是一个ring，设备在其中写入
  //已完成处理（仅每个链条的头部）的描述符编号。
  //有NUM个已使用的环条目。
  struct virtq_used *used;

  //接下来是除了传输数据外的一些其他内容
  char free[NUM];  // 用来表示是否每个描述符是否空闲
  uint16 used_idx; // 表示已经用过的描述符索引

  //操作的跟踪信息，供完成中断到达时使用。同样由链的第一个描述符索引。
  struct {
    struct buf *b;
    char status;
  } info[NUM];

  //磁盘命令头。
  //为了方便起见，一对一使用描述符。
  struct virtio_blk_req ops[NUM];
  
  //磁盘的锁
  struct spinlock vdisk_lock;
  
} __attribute__ ((aligned (PGSIZE))) disk;

struct virtq_desc {
  uint64 addr;
  uint32 len;
  uint16 flags;		
  uint16 next;	//构成链表
};

struct virtq_avail {
  uint16 flags; // 总是为0
  uint16 idx;   //设备即将写入的描述符
  uint16 ring[NUM]; // 链表头的描述符组成的ring
  uint16 unused;
};

struct virtq_used_elem {
  uint32 id;   // 已用过元素链表头的描述符
  uint32 len;
};

struct virtq_used {
  uint16 flags; // 总是为0
  uint16 idx;   // 表示总的使用过的请求
  struct virtq_used_elem ring[NUM];
};

//磁盘请求
struct virtio_blk_req {
  uint32 type; //读或写
  uint32 reserved;	
  uint64 sector;	//块（扇区）
};
```

IO请求采用三个描述符来构成链表，第一个描述符用来表示格式，第二个描述符用来表示块，第三个描述符则用来表示一个单字节状态

```c
 // 第一个描述符表示格式
  disk.desc[idx[0]].addr = (uint64) buf0; //地址为磁盘请求
  disk.desc[idx[0]].len = sizeof(struct virtio_blk_req);  //表示磁盘请求的长度
  disk.desc[idx[0]].flags = VRING_DESC_F_NEXT;  //连接下一个描述符
  disk.desc[idx[0]].next = idx[1];  //下一个描述符为idx[1]

  //第二个描述符用来表示块
  disk.desc[idx[1]].addr = (uint64) b->data;  //地址为块
  disk.desc[idx[1]].len = BSIZE;  //长度为块大小
  if(write)
    disk.desc[idx[1]].flags = 0; // 设备读
  else
    disk.desc[idx[1]].flags = VRING_DESC_F_WRITE; //设备写
  disk.desc[idx[1]].flags |= VRING_DESC_F_NEXT; //读为01，写为11
  disk.desc[idx[1]].next = idx[2];

  //第三个描述符为1个单字节状态
  disk.info[idx[0]].status = 0xff; //  设备成功写入0
  disk.desc[idx[2]].addr = (uint64) &disk.info[idx[0]].status; //设备写入状态的地址
  disk.desc[idx[2]].len = 1;
  disk.desc[idx[2]].flags = VRING_DESC_F_WRITE; // device writes the status
  disk.desc[idx[2]].next = 0;

  // record struct buf for virtio_disk_intr().
  b->disk = 1;  //缓存区内容已经提交给磁盘为0，未完成为1
  disk.info[idx[0]].b = b;

  //告诉磁盘队列中等待的请求的第一个描述符
  disk.avail->ring[disk.avail->idx % NUM] = idx[0];
  //之后等待读写完成的中断
```

xv6通过这样的方式实现了先来先处理的NOOP调度，但由于没有一个直观的请求队列，这种代码阅读起来晦涩难懂，因此我们在virtio_disk_rw之上添加了一层队列调度rw_queue()，在这个函数中实现具体的操作调度算法。

```c
void rw_queue(struct buf*b,int write){

  if(IO_type==0){                               //NOOP
    virtio_disk_rw(b, write);
  }

  else if (IO_type==1){                         //CFQ
    acquire(&queue_lock);
    struct req r;
    r.b=b;
    r.write=write;
    insertMinHeap(disk.heap,r);
    //printf("队列长度%d\n",disk.heap->Size);
    release(&queue_lock);
    virtio_disk_rw(disk.heap->data[1].b, disk.heap->data[1].write);
  }

  else if(IO_type==2){                          //SSTF
    acquire(&queue_lock);
    struct req r;
    r.b=b;
    r.write=write;
    enqueue(sstfq,r);
    release(&queue_lock);
    virtio_disk_rw(sstfq->data[0].b, sstfq->data[0].write);
  }

  else if(IO_type==3){                         //deadline
    acquire(&queue_lock);
    struct req r;
    r.b=b;
    r.write=write;
    Section* pSection = InsertToRing(&disk.ring[write], r);
    Node* pNode = InsertToRBTree(&disk.tree[write], r);
    pSection->pNode = pNode;
    pNode->pSection = pSection;
    release(&queue_lock);
    virtio_disk_rw(disk.readyArea.b, disk.readyArea.write);
  }
  
}
```
### 2.CFQ：

CFQ只需要实现对请求的排序，首先我们构建了请求的数据结构，包括块和读写方式，之后我们根据块号（扇区）实现了一个最小堆，用来对已有的请求排序，在每次请求操作时进行插入操作，在中断发生后进行删除操作，保证堆顶元素为最小值，从而实现按块号的顺序请求。

### 3.SSTF：

SSTF则采用数组来实现，通过distance()函数计算两个块之间的距离，遍历所有请求，找到离当前请求最近的一个请求，放入第二个位置，然后将第一个位置的请求移出数组，实现每次请求的块号都与上一次距离最近。由于SSTF需要遍历请求队列，这一部分有较大的时间消耗的时间，而且维护了一个请求队列，而xv-6系统为半虚拟磁盘，磁头移动的时间消耗不是很明显。因此在测试中SSTF算法可能时间开销较大。
```c
void SSTF(struct Queue* qp)
{
	for(int i=1 ;i< qp->size; i++)    //遍历队列中所有请求
	{
		if (distance(qp->data[0], qp->data[i]) < distance(qp->data[0], qp->data[1])){    //比较请求的物理距离
			struct req cur = qp->data[i];    //将距离最近的替换至队首
			qp->data[i] = qp->data[1];
			qp->data[1] = cur;
		}
	}
}
```
### 4.deadline：

Deadline算法使用红黑树进行排序，实现按块号的顺序请求，而FIFO队列保留请求原有的时间顺序，一旦IO请求的等待时间超过deadline时则优先处理，这样既照顾了效率较高的顺序请求，也有效地防止了请求饥饿。
前述的几种调度算法并未对IO请求的读写方向进行区别对待，其中的NOOP更是事实上倾向于写的，而deadline算法为读写请求分别建立红黑树和FIFO队列，通过为读操作设置更高的优先级且容忍更短的饥饿时间来倾向于写操作。因此，deadline算法能胜任IO任务重且内容单一的情景，如数据库。

### 5.调度算法的切换

利用系统调用IO_schedule改变位于virtio_disk中的变量，来选择不同的调度方式，便于比较分析。具体指令如下：
```c
    IO_schedule noop  //切换为NOOP调度算法
    IO_schedule cfq   //切换为CFQ调度算法
    IO_schedule sstf  //切换为SSTF调度算法
    IO_schedule ddl   //切换为deadline调度算法
```
## 四．测试思路

（代码详见user目录下的IOtest1.c，IOtest2.c，IOtest3.c文件）
 分别输入以下指令进行测试：
 ```c
    IOtest1
    IOtest2
    IOtest3
```
* IOtest1利用父进程和子进程的方式对算法能否顺利通过调度进行了实现，建立串行读写的测试例，规定指定时间10s，测试共能完成多少次读写操作
* IOtest2实现随机读取，对算法进行进一步测试。使用预构建的随机数队列指向不同块位置，在这些不同的位置发送IO请求以测试算法性能，将块输入至随机数队列选定的新的文件中。
* IOtest3为算法性能和特点反应的最终测试。采用了树状方法生成多进程，在叶子进程运行读写代码，并分别检验是否读写成功，属于并行读写。IOtest3记录每一个进程的读写时间，计算其总读写时间，对不同调度方法进行比较。在IOtest3中，输出这些调度请求传输的块的地址信息即可对NOOP和cfq算法、sstf算法、deadline算法进行比较，如果是xv6自带的NOOP算法，所有地址会以原序（即无序）输出，但是在cfq算法进行检定排序的每一个周期中，块的地址应当是有序的（代表请求也被排序）。


## 五．测试结果与分析（待完善）

1） 一些问题
发现不管是用户态还是内核态的程序，都无法使用c标准库，也无法使用malloc分配内存

![img](file:///C:/Users/qinsz/AppData/Local/Temp/msohtmlclip1/01/clip_image014.jpg)

图5无法使用c语言标准库

2） 测试结果
IOtest1：测试指定时间内完成的读写操作（串行读写）
![image](https://user-images.githubusercontent.com/75302896/148913449-aebafc4e-4f12-49a9-ba82-74f8522d73be.png)

IOtest2：测试规定读写次数内的运行耗时（串行读写）
![image](https://user-images.githubusercontent.com/75302896/148914167-d56abc27-0468-46f6-a039-de48785565b3.png)

IOtest3：测试总周转时间、最大周转时间、读写顺序
![image](https://user-images.githubusercontent.com/75302896/148914884-944e8822-d81a-4f33-afcb-67a0aae3aa4e.png)
![image](https://user-images.githubusercontent.com/75302896/148915199-76d421f3-36fd-4251-abc0-dfd23f31cd80.png)

3）结果分析
这部分是我们在自己系统中进行三个测试的测试结果。需要提到的是，由于xv6采用的是半虚拟硬盘而不是机械硬盘，模拟磁头扫动速度太快，不同算法在时间方面的表现差异不大。而我们的算法均额外维护了一个队列，可能导致时间开销反而高于NOOP。


## 六．人员分工（按姓氏排序）

洪宇睿：设计测试方案，deadline算法编写，组织成员讨论，展示ppt制作

卢敏思：熟悉环境配置，sstf算法编写，测试，展示ppt制作，撰写实验报告

覃思中：查找资料，熟悉xv6系统代码与运行过程，实现cfq算法，完善测试用例，完善ssft和deadline算法，展示ppt制作，撰写实验报告，测试

朱景熙：查找资料，撰写实验报告，编写测试用例，展示ppt制作

 
